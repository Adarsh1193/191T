{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We came across different Machine Learning algorithms and learned to implement them with the help of assignments and Kaggle competition. \n",
    "\n",
    "1)\tDecision tree: A decision tree is a decision support tool that uses a tree like graph or model of decisions and their possible consequences, including chance event outcomes, resource cost, and utility. It is a way to display an algorithm that only contains conditional control statements.  \n",
    "2)\tLogistic Regression: Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a binary variable in which there are only two possible outcomes. In logistic regression, the dependent variable is binary, i.e. it only contains data coded as 1 (TRUE, success) or 0 (FALSE, failure).\n",
    "3)\tNeural Networks: Neural networks is a biologically-inspired programming paradigm which enables a computer to learn from observational data. Deep learning is a powerful set of techniques for learning in neural networks. Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing.\n",
    "4)\tSupport Vector Machine: The support vector machine or the support vector networks are supervised learning models with associated learning algorithms that analyze the data used for classification and regression analysis. Given a set of training examples, an SVM training algorithm builds a model that assigns new example making it a non-probabilistic binary linear classifier. \n",
    "\n",
    "The following were the two assignments that were given during the semester which were there on the Kaggle. \n",
    "\n",
    "\n",
    "Assignment 2: Kaggle w/ Titanic\n",
    "Github Link : https://github.com/Adarsh1193/191T/blob/master/Assignment%2B2-%2BKaggle%2Bwith%2BTitanic.ipynb\n",
    "        \n",
    "The data set we use to deduce the classifier for the analysis was composed of 13 features and 891 instances for training, and 418 instances for testing. The features provided by the dataset were survival, ticket class, sex, age in years, the number of siblings/spouse, the number of parents/children, ticket number, passenger fare, cabin number, and Port of Embarkation which help to analyze what sorts of people were likely to survive and predict which passengers survived the tragedy.\n",
    "The steps taken to perform the analysis were following:\n",
    "1.\tImport libraries and Load data set.\n",
    "2.\tReduce data dimension by eliminating irrelevant features (slicing and dicing of data).\n",
    "3.\tPre-processing which involved several steps like Check and Impute missing cases, Discretization via Binning, Convert Discrete Features into Binary, Convert categorical features into numeric etc.\n",
    "4.\tAnalyze features with visualization \n",
    "5.\tValidation Testing and Prediction\n",
    "6.\tSplit data into training and validation sets\n",
    "7.\tPerform Decision Tree Classification\n",
    "8.\tPrediction using Testing set\n",
    "9.\tAnalysis, Results, and Findings\n",
    "The factors considered for prediction were \n",
    "1.\tNumerical Factors \n",
    "a.\tAge (in years)\n",
    "b.\tFare (passenger fare) \n",
    "c.\tSibling/spouse aboard the Titanic\n",
    "d.\tParents or children aboard the titanic\n",
    "2.\tCategorical\n",
    "a.\tTicket Class (1st=1, 2nd=2, 3rd=3)\n",
    "b.\tSex: male, female\n",
    "c.\tPort of Embarkation\n",
    "By performing analysis on the training data, I have achieved the accuracy of 0.70 on the test data.\n",
    "\n",
    "Assignment 3: MNIST\n",
    "Github Link : https://github.com/Adarsh1193/191T/blob/master/MNIST.ipynb\n",
    "        \n",
    "The goal of the assignment was to identify digits from a dataset of tens of thousands of handwritten images using regression to neural network, So I preferred to use TensorFlow library for python as it provides the efficient means to perform the regression models in lesser time.\n",
    "I used following steps to perform the algorithm and draw out the results and got 87% accuracy.\n",
    "\n",
    "1.\tReading the data and looking for any anomalies  then checked whether mnist labels are equally distributed or not .\n",
    "2.\tThen separated labels from training & validation datasets and converted them to categorical values .\n",
    "3.\tNormalized the inputs and extracted size of images for visualization\n",
    "4.\tCreated  placeholder for features and label\n",
    "5.\tCreated a function to initialize Weights with some standard deviation and Bias with a constant.\n",
    "6.\tDefined function conv2d having single strides and same padding.\n",
    "7.\tDefined function max_pool_2x2 having [2x2] strides, same padding and [2x2] window size to shrinks Images to half.\n",
    "8.\tReshaped the vectors.\n",
    "9.\tCalculated and optimized (cost) SoftMax cross entropy between logits and labels.\n",
    "10.\tcorrect prediction - Generating Boolean value based on the comparing predicted and actual labels.\n",
    "11.\taccuracy - Calculated mean of correct prediction\n",
    "12.\tAnd finally generated the output file for the MNIST_output.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
